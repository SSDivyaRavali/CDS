{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "name": "Copy of M5_SNB_MiniProject_4_End_to_End_Analytics_Pyspark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SSDivyaRavali/CDS/blob/main/Module5/Copy_of_M5_SNB_MiniProject_4_End_to_End_Analytics_Pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intensive-feature"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A program by IISc and TalentSprint\n",
        "### Mini-Project: End-to-end analytics application using Pyspark"
      ],
      "id": "intensive-feature"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-kxaHhwXEp9"
      },
      "source": [
        "**DISCLAIMER:** THIS NOTEBOOK IS PROVIDED ONLY AS A REFERENCE SOLUTION NOTEBOOK FOR THE MINI-PROJECT. THERE MAY BE OTHER POSSIBLE APPROACHES/METHODS TO ACHIEVE THE SAME RESULTS."
      ],
      "id": "n-kxaHhwXEp9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buried-qualification"
      },
      "source": [
        "## Problem Statement"
      ],
      "id": "buried-qualification"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smaller-diana"
      },
      "source": [
        "Perform sentiment classification by analyzing the tweets data with Pyspark"
      ],
      "id": "smaller-diana"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opposite-defense"
      },
      "source": [
        "## Learning Objectives"
      ],
      "id": "opposite-defense"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "incoming-professor"
      },
      "source": [
        "At the end of the mini-project, you will be able to :\n",
        "\n",
        "* analyze the text data using pyspark\n",
        "* derive the insights and visualize the data\n",
        "* implement feature extraction and classify the data\n",
        "* train the classification model and deploy"
      ],
      "id": "incoming-professor"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "varied-emission"
      },
      "source": [
        "### Dataset"
      ],
      "id": "varied-emission"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usual-suffering"
      },
      "source": [
        "The dataset chosen for this mini-project is **[Twitter US Airline Sentiment](https://data.world/socialmediadata/twitter-us-airline-sentiment)**. It is a record of tweets about airlines in the US. It was created by scraping Twitter data from February 2015. Contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \"late flight\" or \"rude service\").  Along with other information, it contains ID of a Tweet, the sentiment of a tweet ( neutral, negative and positive), reason for a negative tweet, name of airline and text of a tweet."
      ],
      "id": "usual-suffering"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "original-tsunami"
      },
      "source": [
        "## Information"
      ],
      "id": "original-tsunami"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "premier-northeast"
      },
      "source": [
        "The airline industry is a very competitive market that has grown rapidly in the past 2 decades. Airline companies resort to traditional customer feedback forms which in turn are very tedious and time consuming. This is where Twitter data serves as a good source to gather customer feedback tweets and perform sentiment analysis. This dataset comprises of tweets for 6 major US Airlines and a multi-class classification can be performed to categorize the sentiment (neutral, negative, positive). For this mini-project we will start with pre-processing techniques to clean the tweets and then represent these tweets as vectors. A classification algorithm will be used to predict the sentiment for unseen tweets data. The end-to-end analytics will be performed using Pyspark."
      ],
      "id": "premier-northeast"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BewwTjZaJojg"
      },
      "source": [
        "## Grading = 10 Points"
      ],
      "id": "BewwTjZaJojg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "younger-macro"
      },
      "source": [
        "#### Install Pyspark"
      ],
      "id": "younger-macro"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hxl6vbeA-whf",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "026d7d9d-3488-4421-b5e7-cacab37e7c39"
      },
      "source": [
        "#@title Install packages and download the dataset\n",
        "!pip -qq install pyspark\n",
        "!pip -qq install handyspark\n",
        "!wget -qq https://cdn.iisc.talentsprint.com/CDS/MiniProjects/US_Airline_Tweets.csv\n",
        "print(\"Packages installed successfully and dataset downloaded!!\")"
      ],
      "id": "Hxl6vbeA-whf",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 281.4 MB 24 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 53.1 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Packages installed successfully and dataset downloaded!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rough-battlefield"
      },
      "source": [
        "#### Import required packages"
      ],
      "id": "rough-battlefield"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cheap-workplace"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from handyspark import *\n",
        "#import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "import re\n",
        "import string\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import CountVectorizer\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "id": "cheap-workplace",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "useful-meter",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aa8ad90-6897-494b-cd94-a3155cf246d6"
      },
      "source": [
        "# NLTK imports\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "id": "useful-meter",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pregnant-april"
      },
      "source": [
        "### Data Loading"
      ],
      "id": "pregnant-april"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toxic-baseball"
      },
      "source": [
        "#### Start a Spark Session\n",
        "\n",
        "Spark session is a combined entry point of a Spark application, which came into implementation from Spark 2.0. It provides a way to interact with various Spark functionalities, with a lesser number of constructs."
      ],
      "id": "toxic-baseball"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "christian-rental"
      },
      "source": [
        "spark = SparkSession.builder.appName('TwitterSentiment').getOrCreate()"
      ],
      "id": "christian-rental",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "casual-narrative"
      },
      "source": [
        "#### Load the data and infer the schema\n",
        "\n",
        "To load the dataset use the `read.csv` with `inferSchema` and `header` as parameters."
      ],
      "id": "casual-narrative"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "natural-lexington"
      },
      "source": [
        "dataset = spark.read.csv(\"/content/US_Airline_Tweets.csv\",inferSchema=True,header=True)"
      ],
      "id": "natural-lexington",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alleged-married"
      },
      "source": [
        "dataset.show(1)"
      ],
      "id": "alleged-married",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dulEo4uqrw-6"
      },
      "source": [
        "dataset.count()"
      ],
      "id": "dulEo4uqrw-6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "human-samoa"
      },
      "source": [
        "### EDA & Visualization ( 2 points)"
      ],
      "id": "human-samoa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "angry-canyon"
      },
      "source": [
        "#### Visualize the horizontal barplot of airline_sentiment (positive, negative, neutral)\n",
        "\n",
        "Convert the data to handyspark and remove the other records from the column except 3 values mentioned above and plot the graph"
      ],
      "id": "angry-canyon"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "accomplished-crest"
      },
      "source": [
        "handyDf = dataset.toHandy()\n",
        "\n",
        "handyDf = handyDf.filter(handyDf[\"airline_sentiment\"].isin([\"neutral\",\"positive\",\"negative\"]))\n",
        "sentiments = handyDf.cols['airline_sentiment'][:].value_counts() #.isin(['neutral','positive','negative'])]\n",
        "sentiments.plot.barh()"
      ],
      "id": "accomplished-crest",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEQ-8w9bsAR2"
      },
      "source": [
        "handyDf.count()"
      ],
      "id": "YEQ-8w9bsAR2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "serial-cedar"
      },
      "source": [
        "#### Plot the number of tweets received for each airline"
      ],
      "id": "serial-cedar"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peripheral-bookmark"
      },
      "source": [
        "handyDf.cols['airline'][:].value_counts().plot.bar()"
      ],
      "id": "peripheral-bookmark",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGwC-rM2-plb"
      },
      "source": [
        "#### Visualize a stacked barchart of 6 US airlines and 3 sentiments on each bar\n",
        "\n",
        "* Display the count corresponding to each sentiment in each bar. [hint](https://priteshbgohil.medium.com/stacked-bar-chart-in-python-ddc0781f7d5f)"
      ],
      "id": "jGwC-rM2-plb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "focal-david"
      },
      "source": [
        "grouped = handyDf.groupby('airline','airline_sentiment').agg(count('airline_sentiment'))\n",
        "neutral = grouped.filter(grouped['airline_sentiment']=='neutral').cols['count(airline_sentiment)'][:]\n",
        "positive = grouped.filter(grouped['airline_sentiment']=='positive').cols['count(airline_sentiment)'][:]\n",
        "negative = grouped.filter(grouped['airline_sentiment']=='negative').cols['count(airline_sentiment)'][:]"
      ],
      "id": "focal-david",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "convinced-chance"
      },
      "source": [
        "airlines = list(set(handyDf.cols['airline'][:]))\n",
        "plt.figure(figsize=(10,7))\n",
        "\n",
        "ax1 = plt.bar(airlines,neutral.values,color='y',label='neutral')\n",
        "ax2 = plt.bar(airlines,positive.values,bottom=neutral.values,color='b',label='positive')\n",
        "ax3 = plt.bar(airlines,negative.values,bottom=neutral.values+positive.values,color='r',label='negative')\n",
        "for r1, r2, r3 in zip(ax1, ax2, ax3):\n",
        "    h1 = r1.get_height()\n",
        "    h2 = r2.get_height()\n",
        "    h3 = r3.get_height()\n",
        "    plt.text(r1.get_x() + r1.get_width() / 2., h1 / 2., \"%d\" % h1, ha=\"center\", va=\"center\", color=\"white\", fontsize=16, fontweight=\"bold\")\n",
        "    plt.text(r2.get_x() + r2.get_width() / 2., h1 + h2 / 2., \"%d\" % h2, ha=\"center\", va=\"center\", color=\"white\", fontsize=16, fontweight=\"bold\")\n",
        "    plt.text(r3.get_x() + r3.get_width() / 2., h1 + h2 + h3 / 2., \"%d\" % h3, ha=\"center\", va=\"center\", color=\"white\", fontsize=16, fontweight=\"bold\")\n",
        "plt.legend()#['neutral','positive','negative'])\n",
        "plt.show()"
      ],
      "id": "convinced-chance",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahead-control"
      },
      "source": [
        "#### Visualize the horizontal barplot of negative reasons"
      ],
      "id": "ahead-control"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coordinate-rough"
      },
      "source": [
        "handyDf1 = handyDf.filter(~handyDf[\"negativereason\"].isNull())\n",
        "negativereason = handyDf1.cols['negativereason'][:].value_counts() #.isin(['neutral','positive','negative'])]\n",
        "negativereason.plot.barh()"
      ],
      "id": "coordinate-rough",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scheduled-logistics"
      },
      "source": [
        "### Pre-processing (3 points)"
      ],
      "id": "scheduled-logistics"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pharmaceutical-agency"
      },
      "source": [
        "#### Check the null values and drop the records where the text value is null"
      ],
      "id": "pharmaceutical-agency"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deadly-abraham"
      },
      "source": [
        "# check the null values in text column\n",
        "dataset.filter(dataset.text.isNull()).count()"
      ],
      "id": "deadly-abraham",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "molecular-sword"
      },
      "source": [
        "# filter out and remove null values from text column\n",
        "dataset_filtered = dataset.filter(dataset.text.isNotNull())"
      ],
      "id": "molecular-sword",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auburn-aggregate"
      },
      "source": [
        "# verify the null count\n",
        "dataset.count(), dataset_filtered.count()"
      ],
      "id": "auburn-aggregate",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "convinced-batch"
      },
      "source": [
        "#### Fill the null values with 0 in all the columns except the target\n",
        "\n",
        "The target should not be empty. Ensure that all features are integer type, convert if needed."
      ],
      "id": "convinced-batch"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thorough-jones"
      },
      "source": [
        "fillNull = udf(lambda x: 0 if x == None else x)\n",
        "dataset_filtered = dataset_filtered.withColumn('negativereason_confidence',fillNull(dataset_filtered['negativereason_confidence']).astype('int'))\n",
        "dataset_filtered = dataset_filtered.withColumn('airline_sentiment_confidence',fillNull(dataset_filtered['airline_sentiment_confidence']).astype('int'))"
      ],
      "id": "thorough-jones",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NYdYNat-ipT"
      },
      "source": [
        "#### Preprocessing and cleaning the tweets\n",
        "\n",
        "* Convert the text to lower case\n",
        "* Remove usernames, hashtags and links from the text (tweets)"
      ],
      "id": "8NYdYNat-ipT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "studied-blowing"
      },
      "source": [
        "puncts = \"!#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~\"\n",
        "def words_process(text):\n",
        "    text = text.lower() # lowercase\n",
        "    text.replace(r'http?://[^\\s<>\"]+|www\\.[^\\s<>\"]+', '') # Removing hyperlinks from all the tweets\n",
        "    text.replace('\\d+', '') # Removing numbers from all the tweets\n",
        "    text = \" \".join([i for i in text.split() if i.find(\"@\")== -1]) # removing usernames\n",
        "    text = text.replace('#','') # Removing hashtags, including the text, from all the tweets\n",
        "    text = re.sub(r'[^a-zA-Z0-9 ]',r'',text)\n",
        "    return text\n",
        "\n",
        "words = udf(words_process)\n",
        "dataset_filtered = dataset_filtered.withColumn('text_processed',words(dataset_filtered['text']))\n",
        "dataset_filtered.show()"
      ],
      "id": "studied-blowing",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "subtle-spain"
      },
      "source": [
        "#### Tokenize the text sentence into words using nltk sentence tokenizer"
      ],
      "id": "subtle-spain"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yellow-wholesale"
      },
      "source": [
        "word_udf = udf(lambda x: word_tokenize(x), ArrayType(StringType()))\n",
        "dataset_filtered = dataset_filtered.withColumn(\"wordss\", word_udf(\"text_processed\"))\n",
        "dataset_filtered.show(5)"
      ],
      "id": "yellow-wholesale",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "recognized-wrist"
      },
      "source": [
        "#### Remove the stopwords from tokenized words"
      ],
      "id": "recognized-wrist"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scientific-guinea"
      },
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)"
      ],
      "id": "scientific-guinea",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pleased-skiing"
      },
      "source": [
        "punct_udf1 = udf(lambda x: [w for w in x if not w in stop_words])\n",
        "dataset_filtered = dataset_filtered.withColumn(\"wordss\", punct_udf1(\"wordss\"))\n",
        "\n",
        "array_udf = udf(lambda x: x, ArrayType(StringType()))\n",
        "dataset_filtered = dataset_filtered.withColumn(\"wordss\", array_udf(\"wordss\"))"
      ],
      "id": "pleased-skiing",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "persistent-projection"
      },
      "source": [
        "#### Apply Lemmatization to the words"
      ],
      "id": "persistent-projection"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asian-parade"
      },
      "source": [
        "def lemmatize(text_arr):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(word) for word in text_arr]\n",
        "lem = udf(lemmatize)\n",
        "dataset_filtered = dataset_filtered.withColumn(\"wordss\", lem(\"wordss\"))\n",
        "\n",
        "array_udf = udf(lambda x: x, ArrayType(StringType()))\n",
        "dataset_filtered = dataset_filtered.withColumn(\"wordss\", array_udf(\"wordss\"))"
      ],
      "id": "asian-parade",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "successful-telescope"
      },
      "source": [
        "dataset_filtered.show(5)"
      ],
      "id": "successful-telescope",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR5FnuSn-c-S"
      },
      "source": [
        "### Feature Extraction (3 points)\n",
        "\n",
        "Create the useful features from the text column to train the model\n",
        "\n",
        "For example:\n",
        "* Length of the tweet \n",
        "* No. of hashtags in the tweet starting with '#'\n",
        "* No. of mentions in the tweet starting with '@'\n",
        "\n",
        "Hint: create a new column for each of the above features"
      ],
      "id": "UR5FnuSn-c-S"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "operating-phenomenon"
      },
      "source": [
        "* create a column \"Length of tweet\" using `udf` function"
      ],
      "id": "operating-phenomenon"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adequate-yahoo"
      },
      "source": [
        "# CODE HERE\n",
        "length = udf(lambda x: len(x))\n",
        "\n",
        "dataset_filtered = dataset_filtered.withColumn('tweet_length', length(dataset_filtered['text_processed']).astype('int'))\n",
        "dataset_filtered.show()"
      ],
      "id": "adequate-yahoo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adjustable-knowing"
      },
      "source": [
        "* Create a new column \"No.of Hashtags\" in each tweet"
      ],
      "id": "adjustable-knowing"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unauthorized-venue"
      },
      "source": [
        "num_hashtags = udf(lambda x : len(re.compile(r\"#(\\w+)\").findall(x)))\n",
        "\n",
        "dataset_filtered = dataset_filtered.withColumn('num_hashtags', num_hashtags(dataset_filtered['text']).astype('int'))\n",
        "dataset_filtered.show()"
      ],
      "id": "unauthorized-venue",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "south-upper"
      },
      "source": [
        "* Create a new column \"No.of Mentions\" in each tweet"
      ],
      "id": "south-upper"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sticky-budapest"
      },
      "source": [
        "num_mentions = udf(lambda x : len(re.compile(r\"@(\\w+)\").findall(x)))\n",
        "\n",
        "dataset_filtered = dataset_filtered.withColumn('num_mentions', num_mentions(dataset_filtered['text']).astype('int'))\n",
        "dataset_filtered.show()"
      ],
      "id": "sticky-budapest",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atlantic-installation"
      },
      "source": [
        "* Create a new column \"Punctuation Count\" in each tweet"
      ],
      "id": "atlantic-installation"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "median-norfolk"
      },
      "source": [
        "def punctCount1(text):\n",
        "    sum = 0\n",
        "    for i in text:\n",
        "        if i in \"!$%&'()*+,-./:;<=>?[\\]^_`{|}~\":\n",
        "            sum +=1\n",
        "    return sum\n",
        "\n",
        "punctCount = udf(punctCount1)\n",
        "dataset_filtered = dataset_filtered.withColumn('PunctCount',punctCount(dataset_filtered['text']).astype('int'))\n",
        "dataset_filtered.select('PunctCount').show()"
      ],
      "id": "median-norfolk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bulgarian-packaging"
      },
      "source": [
        "* Create a new column \"Type of Punctuations\" used in each tweet"
      ],
      "id": "bulgarian-packaging"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyric-processor"
      },
      "source": [
        "def types_punctuation(text):\n",
        "    return len(set([i for i in text if i in \"!$%&'()*+,-./:;<=>?[\\]^_`{|}~\"]))\n",
        "\n",
        "typePunct = udf(types_punctuation)\n",
        "dataset_filtered = dataset_filtered.withColumn('typePunct',typePunct(dataset_filtered['text']).astype('int'))\n",
        "dataset_filtered.show(10)"
      ],
      "id": "lyric-processor",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUhFSWtT-XIO"
      },
      "source": [
        "#### Get the features by applying CountVectorizer\n",
        "CountVectorizer converts the list of tokens to vectors of token counts. See the [documentation](https://spark.apache.org/docs/latest/ml-features.html#countvectorizer) for details."
      ],
      "id": "oUhFSWtT-XIO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "christian-voltage"
      },
      "source": [
        "count = CountVectorizer(inputCol=\"wordss\", outputCol=\"rawFeatures\")\n",
        "count_model = count.fit(dataset_filtered)\n",
        "featurizedData = count_model.transform(dataset_filtered)\n",
        "featurizedData.show(truncate=False)"
      ],
      "id": "christian-voltage",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xH0J7Bf-SmB"
      },
      "source": [
        "#### Encode the labels\n",
        "\n",
        "Using the `udf` function encode the string values of *airline_sentiment* to integers."
      ],
      "id": "6xH0J7Bf-SmB"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "assigned-trout"
      },
      "source": [
        "def LabelEncoder(x):\n",
        "    if x == 'positive':\n",
        "        return 0\n",
        "    elif x == 'negative':\n",
        "        return 1\n",
        "    return 2\n",
        "encoder = udf(LabelEncoder)\n",
        "featurizedData = featurizedData.withColumn('label', encoder(featurizedData['airline_sentiment']).astype('int'))\n",
        "featurizedData.show()"
      ],
      "id": "assigned-trout",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vital-sight"
      },
      "source": [
        "### Train the classifier the evaluate (1 point)"
      ],
      "id": "vital-sight"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "attempted-lender"
      },
      "source": [
        "Create vector assembler with the selected features to train the model"
      ],
      "id": "attempted-lender"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stuffed-seating"
      },
      "source": [
        "featureassembler = VectorAssembler(inputCols=['rawFeatures','airline_sentiment_confidence','negativereason_confidence',\n",
        "                                              'tweet_length','num_hashtags','num_mentions','retweet_count','PunctCount','typePunct'] \n",
        "                                   ,outputCol='features')\n",
        "features = featureassembler.transform(featurizedData)\n",
        "features.select('features').show(truncate=False)"
      ],
      "id": "stuffed-seating",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bored-metropolitan"
      },
      "source": [
        "#### Arrange features and label and split them into train and test."
      ],
      "id": "bored-metropolitan"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brazilian-ranking"
      },
      "source": [
        "featuresAndLabels = features.withColumn('labels',featurizedData['label'])\n",
        "final = featuresAndLabels.select('features','labels')\n",
        "final.show()"
      ],
      "id": "brazilian-ranking",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "structured-actress"
      },
      "source": [
        "train_data,test_data = final.randomSplit([0.75,0.25])"
      ],
      "id": "structured-actress",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjmU9f8o-Nme"
      },
      "source": [
        "#### Train the model with train data and make predictions on the test data\n",
        "\n",
        "For classification of text data, implement NaiveBayes classifier. It is a probabilistic machine learning model.\n",
        "\n",
        "For more information about **NaiveBayes Classifier**, click [here](https://spark.apache.org/docs/latest/ml-classification-regression.html#naive-bayes)"
      ],
      "id": "VjmU9f8o-Nme"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "destroyed-religion"
      },
      "source": [
        "nb = NaiveBayes(featuresCol='features', labelCol='labels')\n",
        "model = nb.fit(train_data)"
      ],
      "id": "destroyed-religion",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seeing-money"
      },
      "source": [
        "# get the predictions\n",
        "pred_results = model.transform(test_data)\n",
        "pred_results.select('prediction').show(10)"
      ],
      "id": "seeing-money",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flQXk07t-Kuk"
      },
      "source": [
        "#### Evaluate the model and find the accuracy\n",
        "\n",
        "Compare the labels and predictions and find how many are correct.\n",
        "\n",
        "To find the accuracy, get the count of correct predictions from test data and divide by the total amount of test dataset.\n",
        "\n",
        "**Hint:** convert the predictions dataframe to pandas and compare with labels"
      ],
      "id": "flQXk07t-Kuk"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "greenhouse-crack"
      },
      "source": [
        "# converting to pandas df\n",
        "preds = pred_results.select('labels','prediction').toPandas()\n",
        "preds"
      ],
      "id": "greenhouse-crack",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grateful-iceland"
      },
      "source": [
        "# comparing labels and predictions of test data\n",
        "(preds['labels'] == preds['prediction']).sum() / len(preds)"
      ],
      "id": "grateful-iceland",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a next phase of implementation, we will implement logistic regression model.\n"
      ],
      "metadata": {
        "id": "6ZOKOqvjHDYZ"
      },
      "id": "6ZOKOqvjHDYZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing logistic regression.\n"
      ],
      "metadata": {
        "id": "3NrdGCNSHNZd"
      },
      "id": "3NrdGCNSHNZd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more information about logistic regression, click upon this [button](https://spark.apache.org/docs/2.3.1/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression) to proceed to spark's documentation of logistic regression."
      ],
      "metadata": {
        "id": "t_rCFABiHy3Q"
      },
      "id": "t_rCFABiHy3Q"
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the necessary libraries\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "import numpy\n",
        "from numpy import allclose\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
      ],
      "metadata": {
        "id": "se2Hd0rlIQIv"
      },
      "id": "se2Hd0rlIQIv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating features column through vector assembler\n",
        "assembler = VectorAssembler(inputCols=['rawFeatures','airline_sentiment_confidence','negativereason_confidence',\n",
        "                                              'tweet_length','num_hashtags','num_mentions','retweet_count','PunctCount','typePunct'] \n",
        "                                   ,outputCol='features')\n",
        "df = assembler.transform(featurizedData)\n",
        "df.show()"
      ],
      "metadata": {
        "id": "CwfJ3r2rI5Um"
      },
      "id": "CwfJ3r2rI5Um",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an object of stringindexer\n",
        "label_stringIdx = StringIndexer(inputCol = 'label', outputCol = 'labelIndex')"
      ],
      "metadata": {
        "id": "3bc6y4K-I5RK"
      },
      "id": "3bc6y4K-I5RK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating output column for labels or target variable.\n",
        "df = label_stringIdx.fit(df).transform(df)\n",
        "df.show()"
      ],
      "metadata": {
        "id": "Y8-Kg805I5KJ"
      },
      "id": "Y8-Kg805I5KJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data into train and test\n",
        "train, test = df.randomSplit([0.7, 0.3], seed = 2018)"
      ],
      "metadata": {
        "id": "RyZA-NWnJbdI"
      },
      "id": "RyZA-NWnJbdI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating object of logistic regression\n",
        "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)"
      ],
      "metadata": {
        "id": "MNBFbZaeJcnU"
      },
      "id": "MNBFbZaeJcnU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lrModel = lr.fit(train)"
      ],
      "metadata": {
        "id": "56uYZlHXJcj8"
      },
      "id": "56uYZlHXJcj8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making predictions and evaluating the model"
      ],
      "metadata": {
        "id": "cBnMXt8uJ_r3"
      },
      "id": "cBnMXt8uJ_r3"
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = lrModel.transform(test)"
      ],
      "metadata": {
        "id": "4Rw3IBNwJ1e1"
      },
      "id": "4Rw3IBNwJ1e1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")"
      ],
      "metadata": {
        "id": "7QnagH1ZJ96w"
      },
      "id": "7QnagH1ZJ96w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = evaluator.evaluate(predictions)"
      ],
      "metadata": {
        "id": "WXvYZqlpJ93d"
      },
      "id": "WXvYZqlpJ93d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy = %s\" % (accuracy))"
      ],
      "metadata": {
        "id": "QI-dZMbxJ90o"
      },
      "id": "QI-dZMbxJ90o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qoECylR9J9yG"
      },
      "id": "qoECylR9J9yG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deployment (1 point)\n"
      ],
      "metadata": {
        "id": "NhHs4sweKiHg"
      },
      "id": "NhHs4sweKiHg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deployment is done for both the models:\n",
        "1. Naive bayes\n",
        "2. Logistic regression"
      ],
      "metadata": {
        "id": "aIA3T7UsKlYm"
      },
      "id": "aIA3T7UsKlYm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNK7mTeS8mB4"
      },
      "source": [
        "\n",
        "Let's integrate all the above code snippets in app.py and run it with **Streamlit**.\n",
        "\n",
        "From the start (data loading step), place every code in app.py including data preprocessing, feature extraction and model training.\n",
        "\n",
        "**Similar procedure will be followed for logistic regression model deployment**\n",
        "* In this case, the name of the app file created will be app2.py\n",
        "\n",
        "* implement the `predict_users_Input()` function which takes one tweet input from user and returns the prediction using the trained model.\n",
        "\n",
        "* use the same preprocessing techniques and features extraction used for train data on user input.\n",
        "\n",
        "* user input can be captured from the textbox from **Streamlit** app. Action is triggered when predict button is clicked and user input is classified using `predict_users_Input()` function.\n",
        "\n",
        "For More information about Streamlit, click [here](https://docs.streamlit.io/en/stable/)"
      ],
      "id": "zNK7mTeS8mB4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnlD5cS9zcsL"
      },
      "source": [
        "!pip install -qq streamlit"
      ],
      "id": "wnlD5cS9zcsL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmpuLMdD7ih9"
      },
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "import re\n",
        "import string\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import CountVectorizer\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "from handyspark import *\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "st.write(\"Creating a spark session :heavy_check_mark:\")\n",
        "spark = SparkSession.builder.appName('TwitterSentiment').getOrCreate()\n",
        "dataset = spark.read.csv(\"/content/US_Airline_Tweets.csv\",inferSchema=True,header=True)\n",
        "dataset_filtered = dataset.filter(dataset.text.isNotNull())\n",
        "fillNull = udf(lambda x: 0 if x == None else x)\n",
        "dataset_filtered = dataset_filtered.withColumn('negativereason_confidence',fillNull(dataset_filtered['negativereason_confidence']).astype('int'))\n",
        "dataset_filtered = dataset_filtered.withColumn('airline_sentiment_confidence',fillNull(dataset_filtered['airline_sentiment_confidence']).astype('int'))\n",
        "\n",
        "hdf = dataset.toHandy()\n",
        "\n",
        "def words_process(text):\n",
        "    text = text.lower() # lowercase\n",
        "    text.replace(r'http?://[^\\s<>\"]+|www\\.[^\\s<>\"]+', '') # Removing hyperlinks from all the tweets\n",
        "    text.replace('\\d+', '') # Removing numbers from all the tweets\n",
        "    text = \" \".join([i for i in text.split() if i.find(\"@\")== -1]) # removing usernames\n",
        "    text = text.replace('#','') # Removing hashtags, including the text, from all the tweets\n",
        "    return text\n",
        "\n",
        "words = udf(words_process)\n",
        "dataset_filtered = dataset_filtered.withColumn('text_processed',words(dataset_filtered['text']))\n",
        "\n",
        "word_udf = udf(lambda x: word_tokenize(x), ArrayType(StringType()))\n",
        "dataset_filtered = dataset_filtered.withColumn(\"wordss\", word_udf(\"text_processed\"))\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punct_udf1 = udf(lambda x: [w for w in x if not w in stop_words])\n",
        "dataset_filtered = dataset_filtered.withColumn(\"wordss\", punct_udf1(\"wordss\"))\n",
        "\n",
        "array_udf = udf(lambda x: x, ArrayType(StringType()))\n",
        "dataset_filtered = dataset_filtered.withColumn(\"wordss\", array_udf(\"wordss\"))\n",
        "\n",
        "def lemmatize(text_arr):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(word) for word in text_arr]\n",
        "lem = udf(lemmatize)\n",
        "dataset_filtered = dataset_filtered.withColumn(\"wordss\", lem(\"wordss\"))\n",
        "st.write(\"Preprocessing Done! :heavy_check_mark:\")\n",
        "\n",
        "array_udf = udf(lambda x: x, ArrayType(StringType()))\n",
        "dataset_filtered = dataset_filtered.withColumn(\"wordss\", array_udf(\"wordss\"))\n",
        "\n",
        "length = udf(lambda x: len(x))\n",
        "dataset_filtered = dataset_filtered.withColumn('tweet_length', length(dataset_filtered['text']).astype('int'))\n",
        "num_hashtags = udf(lambda x : len(re.compile(r\"#(\\w+)\").findall(x)) if len(re.compile(r\"#(\\w+)\").findall(x)) > 0 else 0)\n",
        "\n",
        "dataset_filtered = dataset_filtered.withColumn('num_hashtags', num_hashtags(dataset_filtered['text']).astype('int'))\n",
        "num_mentions = udf(lambda x : len(re.compile(r\"@(\\w+)\").findall(x)) if len(re.compile(r\"@(\\w+)\").findall(x)) > 0 else 0)\n",
        "\n",
        "dataset_filtered = dataset_filtered.withColumn('num_mentions', num_mentions(dataset_filtered['text']).astype('int'))\n",
        "def punctCount1(text):\n",
        "    sum = 0\n",
        "    for i in text:\n",
        "        if i in \"!$%&'()*+,-./:;<=>?[\\]^_`{|}~\":\n",
        "            sum +=1\n",
        "    return sum\n",
        "\n",
        "punctCount = udf(punctCount1)\n",
        "\n",
        "dataset_filtered = dataset_filtered.withColumn('PunctCount',punctCount(dataset_filtered['text']).astype('int'))\n",
        "def types_punctuation(text):\n",
        "    return len(set([i for i in text if i in \"!$%&'()*+,-./:;<=>?[\\]^_`{|}~\"]))\n",
        "\n",
        "typePunct = udf(types_punctuation)\n",
        "dataset_filtered = dataset_filtered.withColumn('typePunct',typePunct(dataset_filtered['text']).astype('int'))\n",
        "\n",
        "st.write(\"Ongoing feature extraction!! :heavy_check_mark:\")\n",
        "count = CountVectorizer(inputCol=\"wordss\", outputCol=\"rawFeatures\")\n",
        "count_model = count.fit(dataset_filtered)\n",
        "featurizedData = count_model.transform(dataset_filtered)\n",
        "\n",
        "def LabelEncoder(x):\n",
        "    if x == 'positive':\n",
        "        return 0\n",
        "    elif x == 'negative':\n",
        "        return 1\n",
        "    return 2\n",
        "encoder = udf(LabelEncoder)\n",
        "featurizedData = featurizedData.withColumn('label', encoder(featurizedData['airline_sentiment']).astype('int'))\n",
        "\n",
        "featureassembler = VectorAssembler(inputCols=['rawFeatures','airline_sentiment_confidence','negativereason_confidence',\n",
        "                                              'tweet_length','num_hashtags','num_mentions','retweet_count','PunctCount','typePunct'] \n",
        "                                   ,outputCol='features')\n",
        "features = featureassembler.transform(featurizedData)\n",
        "\n",
        "final = features.withColumn('labels',featurizedData['label'])\n",
        "train_data,test_data = final.randomSplit([0.75,0.25])\n",
        "nb = NaiveBayes(featuresCol='features', labelCol='labels')\n",
        "st.write(\"Training the naive bayes model :heavy_check_mark:\")\n",
        "\n",
        "model = nb.fit(train_data)\n",
        "\n",
        "def predict_users_Input(user_input):\n",
        "  df1 = spark.createDataFrame([ (1, user_input)],['Id', 'UserTweet'])\n",
        "\n",
        "  df1 = df1.withColumn('UserTweet',words(df1['UserTweet']))\n",
        "  df1 = df1.withColumn(\"wordss\", word_udf(\"UserTweet\"))\n",
        "  df1 = df1.withColumn(\"wordss\", punct_udf1(\"wordss\"))\n",
        "  df1 = df1.withColumn(\"wordss\", array_udf(\"wordss\"))\n",
        "  df1 = df1.withColumn(\"wordss\", lem(\"wordss\"))\n",
        "  df1 = df1.withColumn(\"wordss\", array_udf(\"wordss\"))\n",
        "\n",
        "  df1 = df1.withColumn('tweet_length', length(df1['UserTweet']).astype('int'))\n",
        "  df1 = df1.withColumn('num_hashtags', num_hashtags(df1['UserTweet']).astype('int'))\n",
        "  df1 = df1.withColumn('num_mentions', num_mentions(df1['UserTweet']).astype('int'))\n",
        "  df1 = df1.withColumn('PunctCount',punctCount(df1['UserTweet']).astype('int'))\n",
        "  df1 = df1.withColumn('typePunct',typePunct(df1['UserTweet']).astype('int'))\n",
        "  make  = udf(lambda x : 0)\n",
        "  df1 = df1.withColumn('negativereason_confidence',make(df1['UserTweet']).astype('int'))\n",
        "  df1 = df1.withColumn('airline_sentiment_confidence',make(df1['UserTweet']).astype('int'))\n",
        "  df1 = df1.withColumn('retweet_count',make(df1['UserTweet']).astype('int'))\n",
        "  df1_featured = count_model.transform(df1)\n",
        "  test_features = featureassembler.transform(df1_featured)\n",
        "  test_predict = model.transform(test_features)\n",
        "  df_res = test_predict.select('prediction').toPandas()\n",
        "  return df_res\n",
        "\n",
        "def decode(label):\n",
        "  if label == 0:\n",
        "    return \"Positive tweet!\"\n",
        "  elif label == 1:\n",
        "    return \"Negative Tweet!\"\n",
        "  return \"Neutral tweet\"\n",
        "\n",
        "user_input = st.text_input(\"Enter the text input\",\"Your tweet here \")\n",
        "if st.button('predict'):\n",
        "    result = predict_users_Input(user_input)\n",
        "    st.write(decode(result.prediction.values[0]))"
      ],
      "id": "jmpuLMdD7ih9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Writing aap2.py file for logistic regression model"
      ],
      "metadata": {
        "id": "9WMnH48UMKmh"
      },
      "id": "9WMnH48UMKmh"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app2.py\n",
        "import streamlit as st\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "import re\n",
        "import string\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import CountVectorizer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "from handyspark import *\n",
        "import numpy\n",
        "from numpy import allclose\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import os\n",
        "from pyspark.ml.classification import LogisticRegressionModel\n",
        "from pyspark.ml.feature import CountVectorizerModel\n",
        "st.write(\"Creating a spark session :heavy_check_mark:\")\n",
        "spark = SparkSession.builder.appName('TwitterSentiment').getOrCreate()\n",
        "\n",
        "\n",
        "def words_process(text):\n",
        "    text = text.lower() # lowercase\n",
        "    text.replace(r'http?://[^\\s<>\"]+|www\\.[^\\s<>\"]+', '') # Removing hyperlinks from all the tweets\n",
        "    text.replace('\\d+', '') # Removing numbers from all the tweets\n",
        "    text = \" \".join([i for i in text.split() if i.find(\"@\")== -1]) # removing usernames\n",
        "    text = text.replace('#','') # Removing hashtags, including the text, from all the tweets\n",
        "    return text\n",
        "\n",
        "def punctCount1(text):\n",
        "    sum = 0\n",
        "    for i in text:\n",
        "        if i in \"!$%&'()*+,-./:;<=>?[\\]^_`{|}~\":\n",
        "            sum +=1\n",
        "    return sum\n",
        "\n",
        "\n",
        "def LabelEncoder(x):\n",
        "    if x == 'positive':\n",
        "        return 0\n",
        "    elif x == 'negative':\n",
        "        return 1\n",
        "    return 2\n",
        "\n",
        "def lemmatize(text_arr):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(word) for word in text_arr]\n",
        "\n",
        "def types_punctuation(text):\n",
        "    return len(set([i for i in text if i in \"!$%&'()*+,-./:;<=>?[\\]^_`{|}~\"]))\n",
        "\n",
        "\n",
        "\n",
        "words = udf(words_process)\n",
        "word_udf = udf(lambda x: word_tokenize(x), ArrayType(StringType()))\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punct_udf1 = udf(lambda x: [w for w in x if not w in stop_words])\n",
        "array_udf = udf(lambda x: x, ArrayType(StringType()))\n",
        "punctCount = udf(punctCount1)\n",
        "encoder = udf(LabelEncoder)\n",
        "lem = udf(lemmatize)\n",
        "length = udf(lambda x: len(x))\n",
        "array_udf = udf(lambda x: x, ArrayType(StringType()))\n",
        "typePunct = udf(types_punctuation)\n",
        "num_hashtags = udf(lambda x : len(re.compile(r\"#(\\w+)\").findall(x)) if len(re.compile(r\"#(\\w+)\").findall(x)) > 0 else 0)\n",
        "num_mentions = udf(lambda x : len(re.compile(r\"@(\\w+)\").findall(x)) if len(re.compile(r\"@(\\w+)\").findall(x)) > 0 else 0)\n",
        "fillNull = udf(lambda x: 0 if x == None else x)\n",
        "assembler = VectorAssembler(inputCols=['rawFeatures','airline_sentiment_confidence','negativereason_confidence',\n",
        "                                                'tweet_length','num_hashtags','num_mentions','retweet_count','PunctCount','typePunct'] \n",
        "                                    ,outputCol='features')\n",
        "\n",
        "\n",
        "\n",
        "def get_model():\n",
        "  if os.path.exists(\"lrmodel\"):\n",
        "      return LogisticRegressionModel.load('lrmodel'),CountVectorizerModel.load('countmodel')\n",
        "  dataset = spark.read.csv(\"/content/US_Airline_Tweets.csv\",inferSchema=True,header=True)\n",
        "  hdf = dataset.toHandy()\n",
        "  dataset_filtered = dataset.filter(dataset.text.isNotNull())\n",
        "  dataset_filtered = dataset_filtered.withColumn('negativereason_confidence',fillNull(dataset_filtered['negativereason_confidence']).astype('int'))\n",
        "  dataset_filtered = dataset_filtered.withColumn('airline_sentiment_confidence',fillNull(dataset_filtered['airline_sentiment_confidence']).astype('int'))\n",
        "  dataset_filtered = dataset_filtered.withColumn('text_processed',words(dataset_filtered['text']))\n",
        "  dataset_filtered = dataset_filtered.withColumn(\"wordss\", word_udf(\"text_processed\"))\n",
        "  dataset_filtered = dataset_filtered.withColumn(\"wordss\", punct_udf1(\"wordss\"))\n",
        "  dataset_filtered = dataset_filtered.withColumn(\"wordss\", array_udf(\"wordss\"))\n",
        "\n",
        "  dataset_filtered = dataset_filtered.withColumn(\"wordss\", lem(\"wordss\"))\n",
        "  st.write(\"Preprocessing Done! :heavy_check_mark:\")\n",
        "\n",
        "\n",
        "  dataset_filtered = dataset_filtered.withColumn(\"wordss\", array_udf(\"wordss\"))\n",
        "\n",
        "\n",
        "  dataset_filtered = dataset_filtered.withColumn('tweet_length', length(dataset_filtered['text']).astype('int'))\n",
        "\n",
        "  dataset_filtered = dataset_filtered.withColumn('num_hashtags', num_hashtags(dataset_filtered['text']).astype('int'))\n",
        "\n",
        "  dataset_filtered = dataset_filtered.withColumn('num_mentions', num_mentions(dataset_filtered['text']).astype('int'))\n",
        "\n",
        "  dataset_filtered = dataset_filtered.withColumn('PunctCount',punctCount(dataset_filtered['text']).astype('int'))\n",
        "  dataset_filtered = dataset_filtered.withColumn('typePunct',typePunct(dataset_filtered['text']).astype('int'))\n",
        "\n",
        "  st.write(\"Ongoing feature extraction!! :heavy_check_mark:\")\n",
        "  count = CountVectorizer(inputCol=\"wordss\", outputCol=\"rawFeatures\")\n",
        "  count_model = count.fit(dataset_filtered)\n",
        "  count_model.setInputCol(\"wordss\")\n",
        "  count_model.save('countmodel')\n",
        "  featurizedData = count_model.transform(dataset_filtered)\n",
        "\n",
        "\n",
        "  featurizedData = featurizedData.withColumn('label', encoder(featurizedData['airline_sentiment']).astype('int'))\n",
        "\n",
        "  df = assembler.transform(featurizedData)\n",
        "\n",
        "  train, test = df.randomSplit([0.7, 0.3], seed = 2018)\n",
        "  lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
        "\n",
        "  st.write(\"Training the logistic regression model :heavy_check_mark:\")\n",
        "\n",
        "  lrModel = lr.fit(train)\n",
        "  lrModel.save('lrmodel')\n",
        "  return lrModel,count_model\n",
        "\n",
        "lrModel,count_model = get_model()\n",
        "\n",
        "def predict_users_Input(user_input):\n",
        "  df1 = spark.createDataFrame([ (1, user_input)],['Id', 'UserTweet'])\n",
        "\n",
        "  df1 = df1.withColumn('UserTweet',words(df1['UserTweet']))\n",
        "  df1 = df1.withColumn(\"wordss\", word_udf(\"UserTweet\"))\n",
        "  df1 = df1.withColumn(\"wordss\", punct_udf1(\"wordss\"))\n",
        "  df1 = df1.withColumn(\"wordss\", array_udf(\"wordss\"))\n",
        "  df1 = df1.withColumn(\"wordss\", lem(\"wordss\"))\n",
        "  df1 = df1.withColumn(\"wordss\", array_udf(\"wordss\"))\n",
        "\n",
        "  df1 = df1.withColumn('tweet_length', length(df1['UserTweet']).astype('int'))\n",
        "  df1 = df1.withColumn('num_hashtags', num_hashtags(df1['UserTweet']).astype('int'))\n",
        "  df1 = df1.withColumn('num_mentions', num_mentions(df1['UserTweet']).astype('int'))\n",
        "  df1 = df1.withColumn('PunctCount',punctCount(df1['UserTweet']).astype('int'))\n",
        "  df1 = df1.withColumn('typePunct',typePunct(df1['UserTweet']).astype('int'))\n",
        "  make  = udf(lambda x : 0)\n",
        "  df1 = df1.withColumn('negativereason_confidence',make(df1['UserTweet']).astype('int'))\n",
        "  df1 = df1.withColumn('airline_sentiment_confidence',make(df1['UserTweet']).astype('int'))\n",
        "  df1 = df1.withColumn('retweet_count',make(df1['UserTweet']).astype('int'))\n",
        "  df1_featured = count_model.transform(df1)\n",
        "  predictions = lrModel.transform(assembler.transform(df1_featured))\n",
        "  df_res = predictions.select('prediction').toPandas()\n",
        "  return df_res\n",
        "\n",
        "def decode(label):\n",
        "  if label == 0:\n",
        "    return \"Positive tweet!\"\n",
        "  elif label == 1:\n",
        "    return \"Negative Tweet!\"\n",
        "  return \"Neutral tweet\"\n",
        "\n",
        "user_input = st.text_input(\"Enter the text input\",\"Your tweet here \")\n",
        "if st.button('predict'):\n",
        "    result = predict_users_Input(user_input)\n",
        "    st.write(decode(result.prediction.values[0]))"
      ],
      "metadata": {
        "id": "xUqqwJdAMFCp"
      },
      "id": "xUqqwJdAMFCp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To check if the app.py file has written in the current colab sandbox\n",
        "!ls"
      ],
      "metadata": {
        "id": "iqAq9HYtbMab"
      },
      "id": "iqAq9HYtbMab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ngrok\n",
        "\n",
        "Ngrok is a powerful solution for providing secure tunnels from our local system to the public. Here, we first need to signup to [ngrok.com](https://ngrok.com/) and create a free account.\n",
        "\n",
        "**Acount creation is mandatory to get an authentication token**\n",
        "\n",
        "For more information to run Streamlit apps from Colab refer [here](https://medium.com/@jcharistech/how-to-run-streamlit-apps-from-colab-29b969a1bdfc)."
      ],
      "metadata": {
        "id": "c3A-PIjEbjNQ"
      },
      "id": "c3A-PIjEbjNQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install pyngrok, version 4.1.1 is mandatory\n",
        "!pip -qq install pyngrok==4.1.1"
      ],
      "metadata": {
        "id": "iLFWltYPfhfI"
      },
      "id": "iLFWltYPfhfI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import ngrok\n",
        "from pyngrok import ngrok"
      ],
      "metadata": {
        "id": "xUtWGHADfRA4"
      },
      "id": "xUtWGHADfRA4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Get Your Authentication Tokens\n",
        "\n",
        "To use ngrok, you will need an authentication token which can be found on the dashboard of your ngrok [account](https://dashboard.ngrok.com/get-started/setup)."
      ],
      "metadata": {
        "id": "uO2RHzBjfThT"
      },
      "id": "uO2RHzBjfThT"
    },
    {
      "cell_type": "code",
      "source": [
        "#!ngrok authtoken 'paste your authtoken here'\n",
        "\n",
        "!ngrok authtoken 24AzeYQn6rm5KDAA3AH5mh84WpU_3YtjyKe2y3y1F6QbBdijV"
      ],
      "metadata": {
        "id": "v9pQQFpfcd2q"
      },
      "id": "v9pQQFpfcd2q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Start the app\n",
        "\n",
        "Now that we have written our app, we can now start our app like we would have done if we were running it locally. But the caveat is to run it in the background so that if the cell finish running, our app will continue to run as a background process behind."
      ],
      "metadata": {
        "id": "35Uu0FsTaE6D"
      },
      "id": "35Uu0FsTaE6D"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Web URL will be generated only for one app at a time. \n",
        "* If app.py for naive bayes is to be run then(#!streamlit run app2.py&>/dev/null&)the streamlit command used for starting app2.py should be commented out.\n",
        "\n",
        "### Similarly \n",
        "\n",
        "* If app2.py for logistic regression is to be run then(#!streamlit run app.py&>/dev/null&)the streamlit command used for starting app.py should be commented out."
      ],
      "metadata": {
        "id": "JoCyyzoBVi-d"
      },
      "id": "JoCyyzoBVi-d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starting app.py file for naive bayes\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7KyZTzTsMp9C"
      },
      "id": "7KyZTzTsMp9C"
    },
    {
      "cell_type": "code",
      "source": [
        "# Start app\n",
        "!streamlit run app.py&>/dev/null&"
      ],
      "metadata": {
        "id": "Qo1zgZQvICdF"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Qo1zgZQvICdF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starting app2.py file for logistic regression\n"
      ],
      "metadata": {
        "id": "d-7wfYmGMtDd"
      },
      "id": "d-7wfYmGMtDd"
    },
    {
      "cell_type": "code",
      "source": [
        "# Start app\n",
        "!streamlit run app2.py&>/dev/null&"
      ],
      "metadata": {
        "id": "nBqhbZpQMjSi"
      },
      "id": "nBqhbZpQMjSi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will start streamlit on the normal default port of 8501 which we will use for the next section in pyngrok."
      ],
      "metadata": {
        "id": "W73M35dLaN3v"
      },
      "id": "W73M35dLaN3v"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create A Secure Tunnel Using Pyngrok\n",
        "\n",
        "As we learnt earlier ngrok allows us to create a secure tunnel for accessing our local apps and webhooks. We will be using pyngrok which acts as a python wrapper around ngrok.\n",
        "\n",
        "To create our tunnel we will be using pyngrok and passing in the port from streamlit (ie 8501) .\n",
        "\n",
        "After you execute the code below you will get a web app link where you could perform the sentiment prediction task."
      ],
      "metadata": {
        "id": "EhrEGJ1HaUe4"
      },
      "id": "EhrEGJ1HaUe4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate url\n",
        "public_url = ngrok.connect(port='8501')\n",
        "public_url"
      ],
      "metadata": {
        "id": "omN4a9OhIHog"
      },
      "execution_count": null,
      "outputs": [],
      "id": "omN4a9OhIHog"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will generate a public URL that your app will be running on.\n",
        "\n",
        "\n",
        "Refer the screenshot below.\n",
        "![img](https://cdn.iisc.talentsprint.com/CDS/MiniProjects/sentiment_analysis_streamlit_button.JPG)"
      ],
      "metadata": {
        "id": "jge7o3ezaaG_"
      },
      "id": "jge7o3ezaaG_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Shutting Down Your App\n",
        "\n",
        "In case you want to shut down your app, you will have to first kill the streamlit process if it is running on and then shutdown the ngrok from python."
      ],
      "metadata": {
        "id": "7JKcm49xaebg"
      },
      "id": "7JKcm49xaebg"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if streamlit is running\n",
        "!pgrep streamlit"
      ],
      "metadata": {
        "id": "MmLXKUGCLQzE"
      },
      "execution_count": null,
      "outputs": [],
      "id": "MmLXKUGCLQzE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Kill the process by specifying its number from above\n",
        "!kill 2513"
      ],
      "metadata": {
        "id": "nVhoje9tLUpc"
      },
      "execution_count": null,
      "outputs": [],
      "id": "nVhoje9tLUpc"
    },
    {
      "cell_type": "code",
      "source": [
        "# Shutdown ngrok from python\n",
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "5W57gxUkLEqc"
      },
      "execution_count": null,
      "outputs": [],
      "id": "5W57gxUkLEqc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz1JE6qv3JB8"
      },
      "source": [
        "# Python library to run streamlit, flask, fastapi, etc on Google Colab\n",
        "#!pip install -qq colab-everything\n",
        "#from colab_everything import ColabStreamlit\n",
        "#ColabStreamlit('/content/app.py')"
      ],
      "id": "Nz1JE6qv3JB8",
      "execution_count": null,
      "outputs": []
    }
  ]
}